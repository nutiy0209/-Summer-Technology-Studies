A large language model (LLM) is a language model characterized by emergent properties enabled by its large size.[1] They are built with artificial neural networks, (pre-)trained using self-supervised learning and semi-supervised learning, typically containing tens of millions to billions of weights. They are trained using specialized AI accelerator hardware to parallel process vast amounts of text data, mostly scraped from the Internet. As language models, they work by taking an input text and repeatedly predicting the next token or word.[2] The 2017 invention of the transformer architecture drove a series of breakthroughs in LLM development.[3] Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[4][5] Older, specialized supervised models for specific linguistic tasks, have been made largely obsolete by the emergent abilities of LLMs,[4] which are thought to acquire embodied knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[4] Notable LLMs include GPT-4, LLaMa, PaLM, BLOOM, Ernie 3.0 Titan, and Claude.

Towards transformers
Large language models emerged as a consequence of availability of Internet-wide large datasets and parallelizable AI accelerators, initially CPUs, allowing fast enough computation.[6][7]

In 1990, Elman network, in which a recurrent network was trained on simple sentences like "dog chases man", was proposed. The (pre-)trained model was used to convert each word into a vector and the whole vocabulary into a vector database. Groups of vectors were clustered by closeness into a tree. The tree was then found to have a structure. The groups of vectors representing verbs and nouns each belonged to a different large cluster. The noun cluster furtherly was found to have its own sub-clusters for inanimates and animates.[8]
In 1993, the IBM alignment models were used for statistical machine translation.[9]
In 1997, a precursor of large language model, using recurrent neural networks, such as long short-term memory, was proposed.
In 2001, one-billion-word large text corpus, scraped from the Internet, referred to as "very very large" at the time, was used for word disambiguation.[10]
In 2012, AlexNet demonstrated the effectiveness of large neural networks for image recognition, encouraging large artificial neural networks approach instead of older, statistical approaches.
In 2014, a 380M-parameter seq2seq model using two LSTMs networks was proposed by Sutskever at al.[11]
In 2014, a 130M-parameter seq2seq model using a simplified gated recurrent unit was proposed by Bahdanau et al.[12]
In 2014, improving the previous model by using an "additive" kind of attention mechanism in-between two LSTM networks was proposed by Bahdanau et al.[13] It was, however, not yet the parallelizable (scaled "dot product") kind of attention, later proposed in the 2017 transformer paper.
In 2016, Google Translate gradually replaced the older statistical machine translation approach with the newer neural-networks-based approach that included a seq2seq model combined by LSTM and the "additive" kind of attention mechanism. They achieved a higher level of performance than the statistical approach, which took ten years to develop, in only nine months.[14][15]
In 2017, the original (100M-sized) transformer model with a faster (parallelizable or decomposable) attention mechanism was proposed in the "Attention is all you need" paper. Because the model had difficulties converging, it was suggested that the learning rate should be linearly scaled up from 0 to maximal value for the first part of the training (i.e. 2% of the total number of training steps).[16]
In 2018, processing an entire sentence, before assigning each word in it an embedding, had been proposed by the ELMo model. To calculate such, deep contextualized embeddings for each word and additionally for a word before each word, a bi-directional LSTM, trained on a specific task, was used.
In 2018, a bi-directional Transformer has been used in (more than 1B-sized) BERT model.[17]
In 2020, difficulties with converging the original transformer were solved by normalizing layers before (instead of after) multiheaded attention by Xiong et al.[18]
In 2023, an uni-directional (known also as "autoregressive") Transformer has been used in the (more than 100B-sized) GPT-3 and other GPT models.[19][20]
Since 2018, a variation of transformer is the standard architecture in LLMs.[4]

Alternative architectures include the mixture of experts (MoE), which has been proposed by Google, starting with sparsely-gated ones in 2017,[21] Gshard in 2021[22] to GLaM in 2022.[23]